# Jorge Castanon, December 2015
# Data Scientist @ IBM

# run in terminal with:
# ~/Documents/spark-1.5.1/bin/spark-submit visualize-words.py
# Replace this line with:
# /YOUR-SPARK-HOME/bin/spark-submit visualize-words.py

import numpy as np
import math

from pyspark.context import SparkContext

# next 2 lines can be replaced to read from hdfs, 
# if the Word2Vec matrix is big 
Feat = np.load('myW2Vmatrix.npy')    # reads model generated by Word2Vec
words = np.load('myWordList.npy')    # reads list of words

#Feat = np.load('w2v_may1_may19_june1_june11.npy')
#words = np.load('word_may1_may19_june1_june11.npy')


print "\n================================================="
print "Size of the Word2Vec matrix is: ", Feat.shape 
print "Number of words in the models: ", words.shape
print "=================================================\n"

## Spark Context
sc = SparkContext('local','cluster-words') 
# next 3 lines turn some logs off
logger = sc._jvm.org.apache.log4j
logger.LogManager.getLogger("org").setLevel( logger.Level.OFF )
logger.LogManager.getLogger("akka").setLevel( logger.Level.OFF )

## Read the Word2Vec model
# the next line should be read/stored from hdfs if it is large
Feat = sc.parallelize(Feat) 

# map feature matrix to spark vectors
from pyspark.mllib.linalg import Vectors
Feat = Feat.map(lambda vec: (Vectors.dense(vec),))

## Define a df with feature matrix
from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)
dfFeat = sqlContext.createDataFrame(Feat,["features"])
dfFeat.printSchema()

## PCA to project Feature matrix to 2 dimensions
from pyspark.ml.feature import PCA
numComponents = 2
pca = PCA(k=numComponents, inputCol="features", outputCol="pcaFeatures")
model = pca.fit(dfFeat)
dfComp = model.transform(dfFeat).select("pcaFeatures")
# get the first two components to lists to be plotted
maxWordsVis = 30
compX = dfComp.map(lambda vec: vec[0][0]).take(maxWordsVis)
compY = dfComp.map(lambda vec: vec[0][1]).take(maxWordsVis)

print "\n================================================="
print "Variance explained with 2 components is: ", 'Need to be computed'
print "=================================================\n"

## finish Spark session
sc.stop()

## plot
fs=20 #fontsize
w = words[0:maxWordsVis]
import matplotlib.pyplot as plt
fig, ax = plt.subplots()
ax.scatter(compX, compY, color='red')

for i, txt in enumerate(w):
	if(i<50):
		ax.annotate(txt,(compX[i],compY[i]),fontsize=20)


ax.set_xlabel('First Principal Component', fontsize=fs)
ax.set_ylabel('Second Principal Component', fontsize=fs)
ax.set_title('Visualization of Word2Vec via PCA', fontsize=fs)
ax.grid(True)
fig.tight_layout()

plt.show()

