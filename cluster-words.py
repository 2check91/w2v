# Jorge Castanon, October 2015
# Data Scientist @ IBM

# run in terminal with:
# ~/Documents/spark-1.5.1/bin/spark-submit cluster-words.py
# Replace this line with:
# /YOUR-SPARK-HOME/bin/spark-submit cluster-words.py

import numpy as np
from math import sqrt

Feat = np.load('myW2Vmatrix.npy')    # reads model generated by Word2Vec
words = np.load('myWordList.npy')    # reads list of words

print "\n================================================="
print "Size of the W2V matrix is: ", Feat.shape 
print "Number of words in the models: ", words.shape
print "=================================================\n"

## Spark Context
from pyspark.context import SparkContext
sc = SparkContext('local','cluster-words') 
Feat = sc.parallelize(Feat)

## K-means clustering with Spark
from pyspark.mllib.clustering import KMeans
K = 25   # ~ sqrt(n/2) this is a rule of thumb for choosing K,
         # where n is the number of words in the model
         # feel free to choose K with a fancier algorithm
maxiters = 100 # may change depending on the data        
clusters = KMeans.train(Feat, k = K, maxIterations = maxiters) 

print "\n================================================="
print "Number of clusters used: ", K
print "=================================================\n"

## Getting Cluster Labels for each Word and saving to a numpy file
labels =  Feat.map(lambda point: clusters.predict(point)) # add labels to each vector (word)
list_labels = labels.collect()
np.save('myClusters.npy',list_labels)

sc.stop()

